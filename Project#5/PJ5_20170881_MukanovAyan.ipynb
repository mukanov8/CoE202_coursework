{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Activity 5.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSPQ8BjiYxxp",
        "colab_type": "text"
      },
      "source": [
        "# 2019/12/03 CoE 202 Activity 5\n",
        "\n",
        "### **Name Classification**<br/>\n",
        "\n",
        "**Professor: Yong Hoon, Lee**</br>\n",
        "\n",
        "**TA : Seungjun moon, Beomgu Kang**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOSmI5m0Yxxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os.path\n",
        "import string\n",
        "\n",
        "model_save_path = 'tmp/model.ckpt'\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aYrS11hYxx2",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLP9pjPEYxx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate       = 0.005\n",
        "n_epoch             = 200\n",
        "n_hidden            = 128 # hidden layer features\n",
        "max_sequence_length = 19 # maximum number of characters is 19"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRRL6js4Yxx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_input     = len(all_letters)\n",
        "alphabet    = all_letters\n",
        "ethnicities = ['Chinese', 'Japanese', 'Vietnamese', 'Korean', 'Arabic','Czech','Dutch','English','French','German','Greek','Irish','Italian','Polish','Portuguese','Russian','Scottish','Spanish']\n",
        "n_classes   = len(ethnicities) # the number of classes\n",
        "\n",
        "name_strings, ethnicity_strings, str_list, names_list, ethnicity_list = [], [], [], [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaF0I-SEYxyB",
        "colab_type": "text"
      },
      "source": [
        "## Define functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRNqdw71YxyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny-ZRTacYxyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQBfh3iJYxyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def name_one_hot(name, max_sequence_length):\n",
        "    result = []\n",
        "    for char in name:\n",
        "        v = np.zeros(n_input, dtype=np.int) # count space as a character\n",
        "        v[alphabet.index(char)] = 1\n",
        "        result.append(v)\n",
        "    while len(result) < max_sequence_length:\n",
        "        result.append(np.zeros(n_input, dtype=np.int))\n",
        "    result = np.array(result)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3V6enxgYxyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ethnicity_one_hot(ethnicity):\n",
        "    v = np.zeros(n_classes, dtype=np.int)\n",
        "    v[ethnicities.index(ethnicity)] = 1\n",
        "    return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryaum-v3YxyN",
        "colab_type": "text"
      },
      "source": [
        "## Data load "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA_fpRBWYxyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('names_revised.csv', 'r') as csv:\n",
        "    for line in csv:       \n",
        "        l = [s.strip() for s in line.split(',')] # lowercase L, not capital i , l['name', 'ehnicity']\n",
        "        if(l[1] in ethnicities):\n",
        "            name_strings.append(l[0])\n",
        "            ethnicity_strings.append(l[1])\n",
        "            if len(l[0]) > max_sequence_length:\n",
        "                l[0] = l[0][:max_sequence_length]\n",
        "            names_list.append(name_one_hot(l[0], max_sequence_length)) # one-hot vector of each characters of name\n",
        "            ethnicity_list.append(ethnicity_one_hot(l[1])) # one-hot vector of ethnicity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awz6oYxkYxyP",
        "colab_type": "text"
      },
      "source": [
        "## Training - Test Seperation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xYNrUKoYxyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng_state = np.random.get_state() # use the same random number generator state\n",
        "np.random.shuffle(names_list)     # when shuffling the two lists\n",
        "np.random.set_state(rng_state)    # they are effectively shuffled in parallel so that inputs still correspond to outputs after shuffling\n",
        "np.random.shuffle(ethnicity_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBC5yWDAYxyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size = len(names_list) \n",
        "train_size = np.int(size*2/3) \n",
        "\n",
        "training_X = np.array(names_list[:train_size])\n",
        "training_y = np.array(ethnicity_list[:train_size])\n",
        "testing_X = np.array(names_list[train_size:])\n",
        "testing_y = np.array(ethnicity_list[train_size:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJsgeBuOYxyT",
        "colab_type": "text"
      },
      "source": [
        "## Build a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYWrExh2YxyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.placeholder(tf.float32, [None, max_sequence_length, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuIFsA59YxyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_weights = weight_variable([n_hidden, n_classes])\n",
        "out_biases = bias_variable([n_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "cSoKRL00YxyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic RNN\n",
        "#cells = tf.contrib.rnn.BasicRNNCell(num_units = 128)\n",
        "# LSTM\n",
        "#cells = tf.contrib.rnn.BasicLSTMCell(num_units = 128)\n",
        "# GRU\n",
        "#cells = tf.contrib.rnn.GRUCell(num_units = 128)\n",
        "\n",
        "#outputs, states = tf.nn.dynamic_rnn(cells, X, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5zJnA5ZXDuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#y_ = tf.matmul(outputs[:,-1,:], out_weights) + out_biases # predict y based on final rnn output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gcDyjg6YxyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DNN\n",
        "\n",
        "x = tf.reshape(X, [-1, max_sequence_length * n_input])\n",
        "\n",
        "w_init = tf.variance_scaling_initializer()\n",
        "b_init = tf.constant_initializer(0.)\n",
        "\n",
        "\n",
        "## 1st hidden layer\n",
        "w1 = weight_variable([max_sequence_length * n_input,256])         # weight for 1st hidden layer which have 256 units\n",
        "b1 = bias_variable([256])                              # bias for 1st hidden layer which have 256 units\n",
        "h  = tf.matmul(x, w1) + b1                             # matrix multiplication\n",
        "h  = tf.nn.relu(h)                               # relu activation\n",
        "\n",
        "## 2nd hidden layer\n",
        "w2 = weight_variable([256,128])        # weight for 2nd hidden layer which have 256 units\n",
        "b2 = bias_variable([128])                              # bias for 2nd hidden layer which have 256 units\n",
        "h  = tf.matmul(h, w2) + b2                               # matrix multiplication\n",
        "h  = tf.nn.relu(h)                               # relu activation\n",
        "\n",
        "## output layer\n",
        "w3 = weight_variable([128,n_classes])             # weight for output layer which have 256 units\n",
        "y_ = tf.matmul(h, w3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmXxufg_YxyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_, labels=y))\n",
        "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GPMMBbtYxyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation\n",
        "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsA79fukYxyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Softmax\n",
        "pred = tf.nn.softmax(y_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ch1gShxYxyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs9K6gMVYxye",
        "colab_type": "text"
      },
      "source": [
        "## Train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaRT5qAkYxyf",
        "colab_type": "code",
        "outputId": "69a0f9f4-d51b-404b-8f0d-6f29d7b20557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "o7BVuQE9Yxyg",
        "colab_type": "code",
        "outputId": "1ba795df-5cfa-4d81-84a7-a0c81e1135b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "\n",
        "\n",
        "for _ in range(n_epoch+1):\n",
        "    sess.run(train_step, feed_dict={X: training_X, y: training_y})\n",
        "    if _%10 == 0:\n",
        "        train_accuracy = accuracy.eval(feed_dict={X:training_X, y:training_y})\n",
        "        print(\"step %d, training accuracy %g\"%(_, train_accuracy))\n",
        "        test_accuracy = accuracy.eval(feed_dict={X:testing_X, y:testing_y})\n",
        "        print(\"testing accuracy\", test_accuracy)\n",
        "saver.save(sess, model_save_path)\n",
        "print(\"Model saved in file: %s\" % model_save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, training accuracy 0.463677\n",
            "testing accuracy 0.46151546\n",
            "step 10, training accuracy 0.534978\n",
            "testing accuracy 0.5340009\n",
            "step 20, training accuracy 0.668535\n",
            "testing accuracy 0.6596921\n",
            "step 30, training accuracy 0.723692\n",
            "testing accuracy 0.70736814\n",
            "step 40, training accuracy 0.771151\n",
            "testing accuracy 0.7390525\n",
            "step 50, training accuracy 0.809118\n",
            "testing accuracy 0.76341355\n",
            "step 60, training accuracy 0.843722\n",
            "testing accuracy 0.7755194\n",
            "step 70, training accuracy 0.873692\n",
            "testing accuracy 0.7927066\n",
            "step 80, training accuracy 0.89858\n",
            "testing accuracy 0.7995815\n",
            "step 90, training accuracy 0.921599\n",
            "testing accuracy 0.8000299\n",
            "step 100, training accuracy 0.941106\n",
            "testing accuracy 0.7952474\n",
            "step 110, training accuracy 0.954933\n",
            "testing accuracy 0.7921088\n",
            "step 120, training accuracy 0.96562\n",
            "testing accuracy 0.78807354\n",
            "step 130, training accuracy 0.970254\n",
            "testing accuracy 0.78702736\n",
            "step 140, training accuracy 0.974141\n",
            "testing accuracy 0.7861306\n",
            "step 150, training accuracy 0.976457\n",
            "testing accuracy 0.7861306\n",
            "step 160, training accuracy 0.977952\n",
            "testing accuracy 0.7837393\n",
            "step 170, training accuracy 0.978998\n",
            "testing accuracy 0.78179646\n",
            "step 180, training accuracy 0.979447\n",
            "testing accuracy 0.78164697\n",
            "step 190, training accuracy 0.979671\n",
            "testing accuracy 0.78179646\n",
            "step 200, training accuracy 0.979671\n",
            "testing accuracy 0.78015244\n",
            "Model saved in file: tmp/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVE8adV3Yxyh",
        "colab_type": "code",
        "outputId": "57033539-6ac1-4b33-c47f-614d6f8bb1ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "i=0\n",
        "while i<5:\n",
        "    input_name = input('Enter a last name (max 19 letters):')\n",
        "   \n",
        "    while len(input_name) > max_sequence_length or len(input_name) == 0:\n",
        "        input_name = raw_input('Invalid input. Enter a last name (max 19 letters):')\n",
        "   \n",
        "    result=pred.eval(feed_dict={X: np.expand_dims(name_one_hot(input_name, 19), axis=0)})[0]\n",
        "    idx = np.argsort(result)[::-1]\n",
        "    print(\"\\n(%s): %.4f\" % (ethnicities[idx[0]], result[idx[0]]))\n",
        "    print(\"(%s): %.4f\" % (ethnicities[idx[1]], result[idx[1]]))\n",
        "    print(\"(%s): %.4f\" % (ethnicities[idx[2]], result[idx[2]]))\n",
        "    print(\"==========================================\")\n",
        "    i=i+1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a last name (max 19 letters):Amari\n",
            "\n",
            "(Arabic): 0.9967\n",
            "(Japanese): 0.0033\n",
            "(Italian): 0.0000\n",
            "==========================================\n",
            "Enter a last name (max 19 letters):Azimov\n",
            "\n",
            "(Russian): 1.0000\n",
            "(Czech): 0.0000\n",
            "(Polish): 0.0000\n",
            "==========================================\n",
            "Enter a last name (max 19 letters):Wang\n",
            "\n",
            "(German): 0.8282\n",
            "(Chinese): 0.1336\n",
            "(Scottish): 0.0189\n",
            "==========================================\n",
            "Enter a last name (max 19 letters):Bianchi\n",
            "\n",
            "(Italian): 0.9987\n",
            "(Russian): 0.0011\n",
            "(Irish): 0.0001\n",
            "==========================================\n",
            "Enter a last name (max 19 letters):Muller\n",
            "\n",
            "(English): 0.9809\n",
            "(Scottish): 0.0180\n",
            "(French): 0.0005\n",
            "==========================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Com8KsGHYxyj",
        "colab_type": "text"
      },
      "source": [
        "## 4. Report\n",
        "\n",
        "### a. Use GRU, LSTM and Simple RNN functions for training . Compare each of results.\n",
        "\n",
        "### b. Replace the RNN with DNN as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNsqKG2vYxyj",
        "colab_type": "code",
        "outputId": "0def8918-f5c0-4cbd-b6bc-a703f95b7837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''x = tf.reshape(X, [-1, max_sequence_length * n_input])\n",
        "\n",
        "w_init = tf.variance_scaling_initializer()\n",
        "b_init = tf.constant_initializer(0.)\n",
        "\n",
        "## 1st hidden layer\n",
        "w1 =                                # weight for 1st hidden layer which have 256 units\n",
        "b1 =                                # bias for 1st hidden layer which have 256 units\n",
        "h  =                                # matrix multiplication\n",
        "h  =                                # relu activation\n",
        "\n",
        "## 2nd hidden layer\n",
        "w2 =                                # weight for 2nd hidden layer which have 256 units\n",
        "b2 =                                # bias for 2nd hidden layer which have 256 units\n",
        "h  =                                # matrix multiplication\n",
        "h  =                                # relu activation\n",
        "\n",
        "## output layer\n",
        "w3 =                                # weight for output layer which have 256 units\n",
        "\n",
        "y_ = tf.matmul(h, w3)'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'x = tf.reshape(X, [-1, max_sequence_length * n_input])\\n\\nw_init = tf.variance_scaling_initializer()\\nb_init = tf.constant_initializer(0.)\\n\\n## 1st hidden layer\\nw1 =                                # weight for 1st hidden layer which have 256 units\\nb1 =                                # bias for 1st hidden layer which have 256 units\\nh  =                                # matrix multiplication\\nh  =                                # relu activation\\n\\n## 2nd hidden layer\\nw2 =                                # weight for 2nd hidden layer which have 256 units\\nb2 =                                # bias for 2nd hidden layer which have 256 units\\nh  =                                # matrix multiplication\\nh  =                                # relu activation\\n\\n## output layer\\nw3 =                                # weight for output layer which have 256 units\\n\\ny_ = tf.matmul(h, w3)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfIxX2-Yxyk",
        "colab_type": "text"
      },
      "source": [
        "### Submission (Due: Dec. 10 Tue.)\n",
        "Submit your report by Tuesday, December 10 to **\"june1212@kaist.ac.kr\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqnKwkmlYxyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}